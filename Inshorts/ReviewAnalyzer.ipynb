{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qqlu00Wa-dnS"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4uefHGlDPYF",
        "outputId": "0a59637c-c905-4178-cbe3-cd1888fb523e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keybert in /usr/local/lib/python3.10/dist-packages (0.8.4)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.10/dist-packages (from keybert) (2.3.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.25.2)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from keybert) (13.7.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (2.16.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (3.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (2.1.0+cu121)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (23.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.3.8->keybert) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.3.8->keybert) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.3.8->keybert) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=0.3.8->keybert) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=dc4f8ae90dc3857dc789d64d4d4d67950a5ac9663a9a23aab8b2459c8a0576a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWKyZP8cumFC"
      },
      "source": [
        "import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "import pyspark.pandas as pd\n",
        "import re, nltk, spacy, gensim\n",
        "\n",
        "from keybert import KeyBERT\n",
        "import torch.nn.functional as torch_funcs\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "vD-Auw6CE7fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvAJ1iYQ-gBm"
      },
      "source": [
        "### Code - Aspect Based Sentiment Classification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AspectBasedSentimentClassification:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        \"\"\"\n",
        "        Takes in a piece of text, captures the aspect terms and then identifies the sentiment around it.\n",
        "        \"\"\"\n",
        "\n",
        "        self.absa_tokenizer = AutoTokenizer.from_pretrained(\"yangheng/deberta-v3-base-absa-v1.1.\")\n",
        "        self.absa_model = AutoModelForSequenceClassification.from_pretrained(\"yangheng/deberta-v3-base-absa-v1.1.\")\n",
        "        self.sentiment_model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
        "        self.sentiment_model = pipeline(\"sentiment-analysis\", model = self.sentiment_model_path, tokenizer = self.sentiment_model_path)\n",
        "        self.term_extractor = KeyBERT()\n",
        "\n",
        "\n",
        "    def identify_terms(self, txt_):\n",
        "        \"\"\"\n",
        "        Aspect Term Identification using KeyBERT\n",
        "        \"\"\"\n",
        "\n",
        "        terms = self.term_extractor.extract_keywords(txt_, keyphrase_ngram_range = (1, 1), stop_words = \"english\")\n",
        "        return [item[0] for item in terms]\n",
        "\n",
        "\n",
        "    def get_aspects(self, df):\n",
        "        \"\"\"\n",
        "        Identify aspect terms from a piece of text.\n",
        "        \"\"\"\n",
        "\n",
        "        first_run = True\n",
        "        for idx, txt in df.itertuples():\n",
        "            terms = self.identify_termstxt()\n",
        "            x = pd.DataFrame({'cleaned_text': txt, 'aspects': terms})\n",
        "            if first_run:\n",
        "                aspects_df = x.copy()\n",
        "                first_run = False\n",
        "            else:\n",
        "                aspects_df = pd.concat([aspects_df, x], axis = 0)\n",
        "\n",
        "        return aspects_df[['cleaned_text', 'aspects']]\n",
        "\n",
        "\n",
        "    def calculate_sentiment_polarity(self, df):\n",
        "\n",
        "        \"\"\"\n",
        "        Use pretrained models to calculate sentiment polarity, given the aspect terms.\n",
        "        \"\"\"\n",
        "\n",
        "        sentiments, scores = list(), list()\n",
        "        for idx, txt_, asp_ in df.itertuples():\n",
        "            inputs = self.absa_tokenizer(f\"[CLS] {txt_} [SEP] {asp_} [SEP]\", return_tensors = \"pt\")\n",
        "            outputs = self.absa_model(**inputs)\n",
        "            probabilities = torch_funcs.softmax(outputs.logits, dim = 1)\n",
        "            probabilities = probabilities.detach().numpy()[0].tolist()\n",
        "            score = max(probabilities)\n",
        "            sentiment = [\"negative\", \"neutral\", \"positive\"][probabilities.index(score)]\n",
        "            scores.append(score)\n",
        "            sentiments.append(sentiment)\n",
        "\n",
        "        return sentiments, scores\n",
        "\n",
        "\n",
        "    def get_sentiments(self, df):\n",
        "        \"\"\"\"\n",
        "        Calculate the sentiment polarity and score.\n",
        "        \"\"\"\n",
        "        df['sentiment'], df['score'] = self.calculate_sentiment_polarity(df)\n",
        "        return df[['cleaned_text', 'aspects', 'sentiment', 'score']]\n",
        "\n",
        "\n",
        "    def absa(self, data):\n",
        "        \"\"\"\n",
        "        Identify aspect terms from the text.\n",
        "        Calculate the sentiment polarity of the identified terms.\n",
        "        \"\"\"\n",
        "\n",
        "        data = data.groupby(['cleaned_text']).apply(self.get_aspects)\n",
        "        data = data.groupby(['cleaned_text']).apply(self.get_sentiments)\n",
        "\n",
        "        return data"
      ],
      "metadata": {
        "id": "ALJnkwaDE5HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code - Topic Modelling"
      ],
      "metadata": {
        "id": "f7oXTZe_I5Xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TwitterTopicModelling:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        \"\"\"\n",
        "        Performs Topic Modelling on the Twitter dataset.\n",
        "        \"\"\"\n",
        "\n",
        "        nltk.download('stopwords')\n",
        "        self.all_stopwords = stopwords.words('english')\n",
        "        self.all_stopwords.extend(['amp'])\n",
        "        self.spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "    def preprocess_tweets(self, tweet):\n",
        "\n",
        "        \"\"\"\"\n",
        "        Preprocessing the tweets\n",
        "        Block #01: Remove hyperlinks, split hashtags and remove mentions\n",
        "        Block #02: Expand word contractions\n",
        "        Block #03: Simple preprocessing using gensim and Lemmatization using spacy\n",
        "        \"\"\"\n",
        "\n",
        "        tweet = re.sub(r\"http\\S+\", '', tweet)\n",
        "        tweet = re.sub(r\"#[A-Z]{1,}\\s\", lambda m: m.group().lower(), tweet)\n",
        "        tweet = re.sub(r\"#[A-Za-z]\\S*\", lambda m: ' '.join(re.findall('[A-Z][^A-Z]*|[a-z][^A-Z]*', m.group().lstrip('#'))), tweet)\n",
        "        tweet = re.sub(r\"@\\S*\", '', tweet)\n",
        "\n",
        "        tweet = re.sub(\"won\\'t\", \"will not\", tweet)\n",
        "        tweet = re.sub(\"can\\'t\", \"can not\", tweet)\n",
        "        tweet = re.sub(\"shan\\'t\", \"shall not\", tweet)\n",
        "        tweet = re.sub(\"n\\'t\", \" not\", tweet)\n",
        "        tweet = re.sub(\"\\'re\", \" are\", tweet)\n",
        "        tweet = re.sub(\"\\'s\", \" is\", tweet)\n",
        "        tweet = re.sub(\"\\'d\", \" would\", tweet)\n",
        "        tweet = re.sub(\"\\'ll\", \" will\", tweet)\n",
        "        tweet = re.sub(\"\\'ve\", \" have\", tweet)\n",
        "        tweet = re.sub(\"\\'m\", \" am\", tweet)\n",
        "\n",
        "        tweet = re.sub(r\"[^A-Za-z0-9]\", \" \", tweet)\n",
        "        tweet_tok = gensim.utils.simple_preprocess(str(tweet), deacc = True)\n",
        "        tweet = ' '.join([word for word in tweet_tok if word not in self.all_stopwords])\n",
        "        doc = self.spacy_nl(tweet)\n",
        "        tweet_tok = [token.lemma_ for token in doc]\n",
        "\n",
        "        return ' '.join(tweet_tok)\n",
        "\n",
        "\n",
        "    def build_corpus(self, df):\n",
        "        \"\"\"\n",
        "        Build the corpus in the format required by the LDA model\n",
        "        \"\"\"\n",
        "\n",
        "        df = df[df.preprocessed_text != '']\n",
        "        all_cleaned_tweets = [x.split() for x in set(df.preprocessed_text.tolist())]\n",
        "        id2word = gensim.corpora.Dictionary(all_cleaned_tweets)\n",
        "        texts = all_cleaned_tweets\n",
        "        corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "        return id2word, corpus, texts\n",
        "\n",
        "\n",
        "    def build_lda_model(self, corpus_, id2word_, n_topics_, alpha_, eta_):\n",
        "\n",
        "        \"\"\"\n",
        "        Builds the LDA model, given the corpus, BOW and other hyperparameters.\n",
        "        \"\"\"\n",
        "        lda_model = gensim.models.LdaMulticore(corpus = corpus_, id2word = id2word_, num_topics = n_topics_,\n",
        "                                               random_state = 100, chunksize = 100, passes = 10, alpha = alpha_, eta = eta_)\n",
        "        return lda_model\n",
        "\n",
        "\n",
        "    def compute_coherence(self, texts_, corpus_, id2word_, n_topics_, alpha_, eta_):\n",
        "        lda_model = self.build_lda_model(corpus_ = corpus_, id2word_ = id2word_, n_topics_ = n_topics_, alpha_ = alpha_, eta_ = eta_)\n",
        "        coherence_model = gensim.models.CoherenceModel(model = lda_model, texts = texts_, dictionary = id2word_, coherence = 'c_v')\n",
        "        return coherence_model.get_coherence()\n",
        "\n",
        "\n",
        "    def get_optimal_hyperparams(self, df):\n",
        "\n",
        "        \"\"\"\n",
        "        Hyperparameter Tuning\n",
        "        \"\"\"\n",
        "\n",
        "        df['preprocessed_text'] = df['content'].apply(self.preprocess_tweets)\n",
        "        id2word, corpus, texts = self.build_corpus(df)\n",
        "\n",
        "        grid = {'Validation_Set': {}}\n",
        "        hyp_alpha = list(np.arange(0.01, 1, 0.3))\n",
        "        hyp_alpha.append('symmetric')\n",
        "        hyp_alpha.append('asymmetric')\n",
        "        hyp_eta = list(np.arange(0.01, 1, 0.3))\n",
        "        hyp_eta.append('symmetric')\n",
        "        topics_range = range(2, 11, 1)\n",
        "\n",
        "        num_docs = len(corpus)\n",
        "        corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_docs * 0.75)), corpus]\n",
        "        corpus_title = ['75% Corpus', '100% Corpus']\n",
        "\n",
        "        model_results = {'Validation_Set': [], 'Topics': [], 'Alpha': [], 'Beta': [], 'Coherence': []}\n",
        "        pbar = tqdm.tqdm(total = len(hyp_eta) * len(hyp_alpha) * len(topics_range) * len(corpus_title))\n",
        "        for i in range(len(corpus_sets)):\n",
        "            for k in topics_range:\n",
        "                for a in hyp_alpha:\n",
        "                    for e in hyp_eta:\n",
        "                        cv = self.compute_coherence(texts_ = texts, corpus_ = corpus_sets[i], id2word_ = id2word, n_topics = k,\n",
        "                                                    alpha_ = a, eta_ = e)\n",
        "                        model_results['Validation_Set'].append(corpus_title[i])\n",
        "                        model_results['Topics'].append(k)\n",
        "                        model_results['Alpha'].append(a)\n",
        "                        model_results['Eta'].append(e)\n",
        "                        model_results['Coherence'].append(cv)\n",
        "                        pbar.update(1)\n",
        "        model_results_df = pd.DataFrame(model_results)\n",
        "        pbar.close()\n",
        "\n",
        "        return model_results_df\n",
        "\n",
        "\n",
        "        def perform_topic_modelling(self, df):\n",
        "            df['preprocessed_text'] = df['content'].apply(self.preprocess_tweets)\n",
        "            id2word, corpus, texts = self.build_corpus(df)\n",
        "            x_mdl, x_doclda = self.build_lda_model(corpus_ = corpus, id2word_ = id2word, n_topics = 5, alpha_ = 0.05, eta_ = 0.5)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xocv3sVVI4cd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}